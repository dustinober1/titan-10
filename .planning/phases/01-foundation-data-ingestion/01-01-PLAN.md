---
phase: 01-foundation-data-ingestion
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - "pyproject.toml"
  - ".env.example"
  - "src/__init__.py"
  - "src/shared/__init__.py"
  - "src/shared/config.py"
  - "src/shared/types.py"
  - "src/shared/logging.py"
  - "src/storage/__init__.py"
  - "src/storage/models.py"
  - "src/storage/migrations/001_init_hypertables.sql"
  - "src/storage/connection.py"
  - "README.md"
autonomous: true
user_setup: []

must_haves:
  truths:
    - "Python project structure exists with proper module organization"
    - "TimescaleDB hypertable is created with proper schema and compression"
    - "Database connection pool is established and can be queried"
    - "Configuration is loaded from environment with proper validation"
    - "All dependencies are installed and importable"
  artifacts:
    - path: "pyproject.toml"
      provides: "Project dependencies and configuration"
      contains: "[project.dependencies]", "[tool.uv]"
    - path: "src/storage/models.py"
      provides: "Database schema definitions"
      exports: ["create_hypertable_sql", "checkpoints_sql"]
    - path: "src/storage/connection.py"
      provides: "asyncpg connection pool with auto-reconnect"
      exports: ["DatabasePool", "init_pool"]
    - path: "src/shared/config.py"
      provides: "Pydantic settings validation"
      exports: ["Settings"]
    - path: "src/storage/migrations/001_init_hypertables.sql"
      provides: "TimescaleDB hypertable creation"
      contains: "create_hypertable", "compress_segmentby"
  key_links:
    - from: "src/storage/connection.py"
      to: "PostgreSQL/TimescaleDB"
      via: "asyncpg.create_pool with reconnect logic"
      pattern: "asyncpg\\.create_pool"
    - from: "src/storage/connection.py"
      to: "Database connection recovery"
      via: "async reconnect method with retry logic"
      pattern: "async.*reconnect|_ensure_connection"
    - from: "src/shared/config.py"
      to: ".env file"
      via: "pydantic-settings.BaseSettings"
      pattern: "BaseSettings\\_\\_init\\_\\_"
    - from: "src/storage/models.py"
      to: "TimescaleDB"
      via: "SQL migration execution"
      pattern: "execute\\(.*create_hypertable"
---

<objective>
Initialize Python project structure with TimescaleDB hypertable setup for time-series OHLCV data storage.

Purpose: Establish foundation for data ingestion by creating project scaffolding, database schema, and connection infrastructure. This is the bedrock that all data ingestion depends on.
Output: Working project with TimescaleDB hypertables, connection pool with auto-reconnect, and configuration system.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-foundation-data-ingestion/01-RESEARCH.md
@.planning/REQUIREMENTS.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Initialize project structure and dependencies</name>
  <files>
    pyproject.toml
    .env.example
    src/__init__.py
    src/shared/__init__.py
    src/shared/types.py
    README.md
  </files>
  <action>
    Initialize Python project with uv package manager:

    1. Create pyproject.toml with:
       - Project metadata (name: titan-10, version: 0.1.0)
       - Dependencies from research (ccxt>=4.5, asyncpg, pydantic>=2.0, pydantic-settings>=2.0, python-dotenv, tenacity>=4.4.0, apscheduler>=3.10)
       - Dev dependencies (pytest, pytest-asyncio, ruff, mypy)
       - [tool.uv] configuration for dependency management
       - [tool.ruff] configuration for linting

    2. Create .env.example with all required environment variables:
       - DATABASE_URL (postgresql://user:pass@host:port/db)
       - EXCHANGE_API_KEYS (JSON object of exchange credentials)
       - LOG_LEVEL (default: INFO)
       - TOP_SYMBOLS (comma-separated list of top 20 crypto pairs)

    3. Create module structure:
       - src/__init__.py (empty)
       - src/shared/__init__.py (export config, types, logging)
       - src/shared/types.py (define OHLCV, Exchange enums, type aliases)

    4. Update README.md with:
       - Project description
       - Prerequisites (Python 3.11+, PostgreSQL 16+ with TimescaleDB)
       - Installation instructions (uv sync, cp .env.example .env)
       - Quick start guide

    DO NOT use virtualenv or pip - use uv exclusively. DO NOT hardcode any credentials.
  </action>
  <verify>
    Run: uv sync
    Check: Python can import all dependencies (import ccxt, import asyncpg, import pydantic)
  </verify>
  <done>
    pyproject.toml exists with all dependencies, uv sync completes successfully, all modules are importable
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement configuration and logging systems</name>
  <files>
    src/shared/config.py
    src/shared/logging.py
  </files>
  <action>
    Create configuration and logging infrastructure using patterns from research:

    1. src/shared/config.py:
       - Define Settings class extending pydantic_settings.BaseSettings
       - Fields: database_url (str), log_level (str = "INFO"), top_symbols (list[str]), exchange_credentials (dict[str, dict])
       - Use pydantic.Field for validation (gt=0 for numeric fields where applicable)
       - Add validator for top_symbols to ensure proper format (e.g., "BTC/USDT")
       - Implement get_settings() singleton function
       - DO NOT log secrets or credentials

    2. src/shared/logging.py:
       - Configure structured logging using Python's logging module
       - Format: timestamp [level] module: message
       - Add correlation_id filter for request tracing (placeholder for Phase 4)
       - Support both file and console handlers
       - Use UTC timestamps for all log entries

    Reference: Pattern 1 from research (Pydantic settings), timezone consistency (DATA-07)
  </action>
  <verify>
    Run: python -c "from src.shared.config import get_settings; s = get_settings(); print(s.database_url)"
    Check: Settings loads from .env without errors
  </verify>
  <done>
    Settings class validates all environment variables, logging produces structured UTC output
  </done>
</task>

<task type="auto">
  <name>Task 3: Create TimescaleDB hypertable schema and migrations</name>
  <files>
    src/storage/models.py
    src/storage/migrations/001_init_hypertables.sql
  </files>
  <action>
    Create TimescaleDB hypertable schema following Pattern 4 from research:

    1. src/storage/models.py:
       - Define SQL schema strings as module constants
       - ohlcv_table_sql: CREATE TABLE with TIMESTAMPTZ, symbol, exchange, OHLCV columns, PRIMARY KEY (time, symbol, exchange)
       - create_hypertable_sql: SELECT create_hypertable('ohlcv', 'time', chunk_time_interval => INTERVAL '1 day')
       - indexes_sql: CREATE INDEX on (symbol, time DESC) and (exchange, time DESC)
       - compression_sql: ALTER TABLE SET (timescaledb.compress, compress_orderby = 'time', compress_interval = '7 days')
       - checkpoints_table_sql: CREATE TABLE backfill_checkpoints (symbol VARCHAR(20) PRIMARY KEY, last_timestamp BIGINT, updated_at TIMESTAMPTZ)

    2. src/storage/migrations/001_init_hypertables.sql:
       - Combine all SQL statements in execution order
       - Add IF NOT EXISTS checks for idempotency
       - Include comments explaining each section
       - DO NOT use compress_segmentby on 'symbol' (high cardinality anti-pattern from research)

    Reference: Pitfall 3 (compression performance), Pattern 4 (hypertable setup)
  </action>
  <verify>
    Run: cat src/storage/migrations/001_init_hypertables.sql | grep -E "(create_hypertable|create_table|compress)"
    Check: SQL contains hypertable creation, indexes, compression policy
  </verify>
  <done>
    Migration SQL creates hypertable with proper compression, avoids high-cardinality segmentby
  </done>
</task>

<task type="auto">
  <name>Task 4: Implement asyncpg connection pool with auto-reconnect</name>
  <files>
    src/storage/__init__.py
    src/storage/connection.py
  </files>
  <action>
    Create database connection pool with auto-reconnect following Pattern 5 from research:

    1. src/storage/connection.py:
       - Define DatabasePool class with __init__(dsn: str)
       - async init() method: create asyncpg.create_pool with min_size=5, max_size=20, command_timeout=60, max_queries=50000, max_inactive_connection_lifetime=300.0
       - async insert_ohlcv() method: acquire connection, use executemany with INSERT ... ON CONFLICT DO NOTHING
       - async save_checkpoint() method: INSERT INTO backfill_checkpoints with ON CONFLICT UPDATE
       - async get_last_checkpoint() method: SELECT for resume capability
       - async close() method: close pool
       - Add error handling for pool timeout and connection errors
       - CRITICAL: Add async reconnect method for AUTO-02 requirement:
         * Define async _ensure_connection method: checks if pool is healthy, reinitializes if needed
         * Define async reconnect method: close existing pool, create new pool with exponential backoff retry (max 5 attempts, 1s base delay)
         * Call _ensure_connection at start of each query method (insert_ohlcv, save_checkpoint, get_last_checkpoint)
         * Log reconnection attempts and successes

    2. src/storage/__init__.py:
       - Export DatabasePool, create_pool function
       - Define module-level pool variable for singleton pattern

    Reference: Pattern 5 (asyncpg pool), Pitfall 5 (connection pool exhaustion), AUTO-02 requirement (auto-recover from database connection loss)
  </action>
  <verify>
    Run: python -c "import asyncio; from src.storage.connection import DatabasePool; asyncio.run(DatabasePool('postgresql://localhost/test').init())"
    Check: Pool initializes without errors (database not required to exist yet)
    Run: python -c "from src.storage.connection import DatabasePool; import inspect; print('reconnect' in inspect.getsource(DatabasePool))"
    Check: DatabasePool has reconnect method for auto-recovery
  </verify>
  <done>
    DatabasePool class creates pool with proper sizing, implements insert_ohlcv and checkpoint methods, includes async reconnect for auto-recovery
  </done>
</task>

</tasks>

<verification>
## Plan Verification

1. **Dependency Installation**: Run `uv sync` and verify all packages install without errors
2. **Configuration**: Run `python -c "from src.shared.config import get_settings; print(get_settings())"` and confirm settings load
3. **Schema Validation**: Run `cat src/storage/migrations/001_init_hypertables.sql | psql -f -` (when database available) to verify SQL syntax
4. **Connection Pool**: Run `python -c "from src.storage.connection import DatabasePool; print(DatabasePool.__doc__)"` to confirm module loads
5. **Auto-Reconnect**: Run `python -c "from src.storage.connection import DatabasePool; import inspect; print('reconnect' in inspect.getsource(DatabasePool))"` to verify reconnect method exists

## Success Criteria
- [ ] pyproject.toml contains all dependencies from research
- [ ] .env.example documents all required environment variables
- [ ] Settings class validates configuration without hardcoding credentials
- [ ] Hypertable SQL creates time-series optimized table with compression
- [ ] DatabasePool implements async connection pooling with proper sizing
- [ ] DatabasePool includes async reconnect method with retry logic for AUTO-02
- [ ] All modules are importable without errors
</verification>

<success_criteria>
1. Python project structure exists with src/ directory containing shared, storage submodules
2. All dependencies (ccxt, asyncpg, pydantic, tenacity, apscheduler) installable via uv sync
3. TimescaleDB hypertable migration SQL creates compressed, partitioned table structure
4. Database connection pool can be instantiated and configured via Settings
5. Configuration system loads from .env with Pydantic validation
6. DatabasePool implements auto-reconnect with exponential backoff retry (addresses AUTO-02)
7. README documents setup, installation, and quick start instructions
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation-data-ingestion/01-01-SUMMARY.md` with:
- Tech stack added (ccxt, asyncpg, pydantic, tenacity, apscheduler)
- Key files created (connection.py with auto-reconnect, models.py, config.py)
- Auto-reconnect implementation details (reconnect method, exponential backoff)
- Any deviations from research patterns
- Next steps ready for 01-02 (CCXT integration)
</output>
