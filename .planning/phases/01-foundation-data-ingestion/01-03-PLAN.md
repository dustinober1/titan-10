---
phase: 01-foundation-data-ingestion
plan: 03
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - "src/ingestor/backfill.py"
  - "src/storage/queries.py"
autonomous: false
user_setup: []

must_haves:
  truths:
    - "System backfills 5 years of historical daily candle data on first startup"
    - "Backfill progress is saved to checkpoints after each batch"
    - "Failed backfill jobs resume from last checkpoint (not from beginning)"
    - "Backfill data is validated before insertion to TimescaleDB"
    - "Backfill completes without API rate limit violations"
  artifacts:
    - path: "src/ingestor/backfill.py"
      provides: "Historical backfill with checkpoint-based recovery"
      exports: ["BackfillManager", "backfill_all_symbols"]
    - path: "src/storage/queries.py"
      provides: "SQL query helpers for gap detection and data retrieval"
      exports: ["detect_gaps", "get_latest_timestamp"]
  key_links:
    - from: "src/ingestor/backfill.py"
      to: "src/storage/connection.py"
      via: "DatabasePool.insert_ohlcv and checkpoint methods"
      pattern: "await.*storage\\.insert_ohlcv\\(.*\\)"
    - from: "src/ingestor/backfill.py"
      to: "backfill_checkpoints table"
      via: "checkpoint save/load operations"
      pattern: "save_checkpoint|get_last_checkpoint"
    - from: "src/storage/queries.py"
      to: "TimescaleDB ohlcv hypertable"
      via: "gap detection SQL queries"
      pattern: "generate_series|LEFT JOIN.*NULL"
---

<objective>
Implement historical data backfill with checkpoint-based recovery for 5 years of daily candle data.

Purpose: Populate TimescaleDB with historical cryptocurrency market data enabling long-term analysis. Checkpoint-based recovery ensures multi-hour backfill jobs can resume after failures without losing progress.
Output: Working backfill system that fetches, validates, and stores 5 years of historical OHLCV data with resume capability.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-foundation-data-ingestion/01-RESEARCH.md
@.planning/REQUIREMENTS.md
@.planning/phases/01-foundation-data-ingestion/01-01-PLAN.md
@.planning/phases/01-foundation-data-ingestion/01-02-PLAN.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement BackfillManager with checkpoint tracking</name>
  <files>
    src/ingestor/backfill.py
  </files>
  <action>
    Create BackfillManager class following Pattern 2 from research:

    1. src/ingestor/backfill.py:
       - Import ExchangeWrapper from src.ingestor.exchange
       - Import DatabasePool from src.storage.connection
       - Import OHLCVData from src.ingestor.normalizer
       - Import datetime, timedelta, asyncio, logging
       - Define BackfillManager class with __init__(exchange_id: str, storage: DatabasePool)
       - In __init__: create ExchangeWrapper instance, initialize empty checkpoints dict
       - Define async backfill_symbol method: symbol (str), timeframe (str='1d'), years_back (int=5)
       - In backfill_symbol:
         * Check for existing checkpoint: call storage.get_last_checkpoint(symbol)
         * If checkpoint exists: start_date = datetime.fromtimestamp(last_timestamp / 1000), log resume message
         * If no checkpoint: start_date = datetime.now() - timedelta(days=years_back * 365)
         * Initialize current_since = int(start_date.timestamp() * 1000), all_candles = []
         * Enter while True loop:
           - Fetch candles via await self.exchange.fetch_ohlcv(symbol, timeframe, since=current_since, limit=1000)
           - Break if not candles returned
           - Extend all_candles with fetched data
           - Update checkpoint: last_ts = candles[-1][0], self.checkpoints[symbol] = last_ts, await storage.save_checkpoint(symbol, last_ts)
           - Move to next batch: current_since = last_ts + 1
           - Await asyncio.sleep(1) for rate limit safety
           - Catch exceptions: log error, break loop (checkpoint saved for resume)
         * After loop: bulk insert via await storage.insert_ohlcv(symbol, all_candles)
         * Log completion message with candle count

    Reference: Pattern 2 (checkpoints), Pitfall 4 (backfill without checkpoints), Success Criteria (5 years historical)
  </action>
  <verify>
    Run: python -c "from src.ingestor.backfill import BackfillManager; print(BackfillManager.__doc__)"
    Check: BackfillManager has backfill_symbol method with checkpoint logic
  </verify>
  <done>
    BackfillManager saves checkpoints after each batch, can resume from last checkpoint on restart
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement multi-symbol backfill orchestration</name>
  <files>
    src/ingestor/backfill.py
  </files>
  <action>
    Add orchestration functions to src/ingestor/backfill.py:

    1. Add to src/ingestor/backfill.py:
       - Define async backfill_all_symbols function accepting: symbols (list[str]), exchange_id (str), storage (DatabasePool), years_back (int=5)
       - In backfill_all_symbols:
         * Create BackfillManager instance with exchange_id and storage
         * Iterate through symbols list
         * For each symbol: await manager.backfill_symbol(symbol, '1d', years_back)
         * Log completion status per symbol
         * Handle exceptions per symbol (log error, continue to next symbol)
         * Log overall completion message
       - Define async initial_backfill function accepting: storage (DatabasePool)
       - In initial_backfill:
         * Import get_settings from src.shared.config
         * Get top_symbols from settings
         * Call backfill_all_symbols(symbols=top_symbols, exchange_id='binance', storage=storage, years_back=5)
         * This is the main entry point called on system startup

    Reference: Pattern 3 (APScheduler), Success Criteria (backfill on first startup)
  </action>
  <verify>
    Run: python -c "from src.ingestor.backfill import backfill_all_symbols, initial_backfill; print(initial_backfill.__doc__)"
    Check: Functions exist with correct signatures
  </verify>
  <done>
    backfill_all_symbols orchestrates multi-symbol backfill, initial_backfill is main startup entry point
  </done>
</task>

<task type="auto">
  <name>Task 3: Implement gap detection queries</name>
  <files>
    src/storage/queries.py
  </files>
  <action>
    Create gap detection module following Gap Detection Query example from research:

    1. src/storage/queries.py:
       - Define async detect_gaps function accepting: pool (DatabasePool), symbol (str), timeframe (str='1d')
       - In detect_gaps:
         * Acquire connection from pool
         * For daily timeframe (timeframe == '1d'):
           - Use generate_series to create expected time series from MIN(time) to MAX(time)
           - LEFT JOIN with actual candle timestamps
           - Filter WHERE actual_times.time IS NULL (missing candles)
           - Execute query: SELECT expected_times.time FROM expected_times LEFT JOIN actual_times ON expected_times.time = actual_times.time WHERE actual_times.time IS NULL ORDER BY expected_times.time
         * Fetch all results
         * Log warning if gaps found: "Found {len(gaps)} missing candles for {symbol}"
         * Return gaps list
       - Define async get_latest_timestamp function: pool, symbol
       - In get_latest_timestamp:
         * Query: SELECT MAX(time) FROM ohlcv WHERE symbol = $1
         * Return max timestamp or None if no data

    Reference: Gap Detection Query example, Pitfall 2 (missing candle data), DATA-06 requirement
  </action>
  <verify>
    Run: python -c "from src.storage.queries import detect_gaps, get_latest_timestamp; print(detect_gaps.__doc__)"
    Check: Functions exist with proper SQL query patterns
  </verify>
  <done>
    detect_gaps identifies missing candles using TimescaleDB time-series functions, get_latest_timestamp retrieves newest data point
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>Complete historical backfill system with checkpoint-based recovery</what-built>
  <how-to-verify>
    1. Ensure TimescaleDB is running and accessible (DATABASE_URL in .env)
    2. Run initial backfill: `python -c "import asyncio; from src.ingestor.backfill import initial_backfill; from src.storage.connection import DatabasePool; from src.shared.config import get_settings; async def run(): pool = DatabasePool(get_settings().database_url); await pool.init(); await initial_backfill(pool); await pool.close(); asyncio.run(run())"`
    3. Monitor logs for:
       - Checkpoint save messages (every 1000 candles)
       - Progress indicators per symbol
       - Completion message with total candle count
    4. Verify data in database: Run `psql -c "SELECT symbol, COUNT(*), MIN(time), MAX(time) FROM ohlcv GROUP BY symbol;"`
    5. Test checkpoint recovery: Interrupt backfill (Ctrl+C), restart, confirm "Resuming {symbol} from {date}" message
    6. Run gap detection: `python -c "import asyncio; from src.storage.queries import detect_gaps; from src.storage.connection import DatabasePool; from src.shared.config import get_settings; async def run(): pool = DatabasePool(get_settings().database_url); await pool.init(); gaps = await detect_gaps(pool, 'BTC/USDT'); print(f'Gaps: {len(gaps)}'); await pool.close(); asyncio.run(run())"`
  </how-to-verify>
  <resume-signal>Type "approved" if backfill completes successfully with checkpoints, or describe any issues</resume-signal>
</task>

</tasks>

<verification>
## Automated Verification (Before Checkpoint)

1. **Module Import**: Run `python -c "from src.ingestor.backfill import BackfillManager, backfill_all_symbols, initial_backfill"` to confirm modules load
2. **Checkpoint Logic**: Run `python -c "from src.ingestor.backfill import BackfillManager; import inspect; print(inspect.getsource(BackfillManager.backfill_symbol))"` to verify checkpoint save calls
3. **Gap Detection**: Run `python -c "from src.storage.queries import detect_gaps; import inspect; print('generate_series' in inspect.getsource(detect_gaps))"` to confirm gap detection SQL

## Checkpoint Verification (Human)

4. **Backfill Execution**: Run initial_backfill and confirm:
   - Checkpoints are saved after each batch
   - Progress is logged per symbol
   - Database contains historical data
5. **Recovery Test**: Interrupt and restart to confirm resume capability
6. **Gap Detection**: Verify detect_gaps identifies missing candles

## Success Criteria
- [ ] BackfillManager saves checkpoints after each 1000-candle batch
- [ ] Checkpoints are stored in backfill_checkpoints table
- [ ] Failed backfill resumes from last checkpoint (not beginning)
- [ ] 5 years of historical data is fetched per symbol
- [ ] Gap detection query identifies missing candles
- [ ] Rate limiting prevents API violations (1s sleep between batches)
</verification>

<success_criteria>
1. BackfillManager implements incremental backfill with checkpoint tracking
2. Checkpoints are saved to database after each batch (enables recovery)
3. initial_backfill function orchestrates multi-symbol backfill on startup
4. Gap detection query uses generate_series for missing candle identification
5. asyncio.sleep(1) prevents rate limit violations between batches
6. Error handling allows per-symbol failures without stopping entire backfill
7. Human verification confirms 5 years of data is stored successfully
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation-data-ingestion/01-03-SUMMARY.md` with:
- Backfill implementation status (checkpoint system, batch size)
- Recovery mechanism details (checkpoint table, resume logic)
- Gap detection implementation
- Manual verification results (backfill duration, data quality)
- Next steps ready for 01-04 (multi-exchange redundancy)
</output>
